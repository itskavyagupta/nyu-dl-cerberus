{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c49d079e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-12T23:02:01.728588Z",
     "iopub.status.busy": "2024-04-12T23:02:01.728265Z",
     "iopub.status.idle": "2024-04-12T23:02:01.740716Z",
     "shell.execute_reply": "2024-04-12T23:02:01.740056Z"
    },
    "id": "f9feviImC5sG",
    "papermill": {
     "duration": 0.025282,
     "end_time": "2024-04-12T23:02:01.742549",
     "exception": false,
     "start_time": "2024-04-12T23:02:01.717267",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parameters that can be changed according to the environment, number of epochs, learning rate, etc.\n",
    "\n",
    "# To choose kaggle vs colab notebooks\n",
    "kaggle = 1 # 1 if kaggle, 0 if colab\n",
    "load_dataset_online = 0 # 0 if kaggle, as the Kaggle competition already had the dataset, 1 to download teh dataset\n",
    "use_wandb = 0 # 1 to use wandb (Weights And Biases) and save our sweeps to wandb, 0 to not run wandb\n",
    "wandbapi = '' ## add wandb API key from wandb.com/authorize, otherwise wandb won't work\n",
    "\n",
    "# model config to pass to our main model\n",
    "blocks = [3,5,3]\n",
    "channels = [64,128,256]\n",
    "\n",
    "earlystop = 0 # to use early stop for our training\n",
    "\n",
    "#number of epochs\n",
    "number_of_epochs = 200 # max epochs\n",
    "save_freq = 15 # number of epochs after which we save our model\n",
    "\n",
    "#early stopping criterion\n",
    "earlystop_patience = 5\n",
    "earlystop_mindelta = 2\n",
    "earlystop_threshold = 88\n",
    "\n",
    "# to create csv\n",
    "final_test_create_csv=0\n",
    "\n",
    "# hyperparameters if not using wandb\n",
    "args_weight_decay = 5e-4\n",
    "args_lr = 0.01\n",
    "args_optimizer = 'sgd' # options: sgd, sgdn, adadelta-clipping,\n",
    "\n",
    "# load model\n",
    "load_model = 0\n",
    "load_model_name = '100_checkpoint.tar' #can only be done on colab\n",
    "\n",
    "# other parameters\n",
    "our_batch_size = 128\n",
    "our_number_workers = 2\n",
    "\n",
    "# Maximum number of iterations for annealing\n",
    "t_max = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba04b375",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-04-12T23:02:01.761885Z",
     "iopub.status.busy": "2024-04-12T23:02:01.761616Z",
     "iopub.status.idle": "2024-04-12T23:02:25.510217Z",
     "shell.execute_reply": "2024-04-12T23:02:25.509199Z"
    },
    "id": "CdKFRqDQCekk",
    "outputId": "2e993186-6d0f-4f13-c1e3-e67cd112c8f1",
    "papermill": {
     "duration": 23.760762,
     "end_time": "2024-04-12T23:02:25.512637",
     "exception": false,
     "start_time": "2024-04-12T23:02:01.751875",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import pickle\n",
    "\n",
    "import argparse\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import pprint\n",
    "\n",
    "# from utils import progress_bar\n",
    "from time import perf_counter\n",
    "\n",
    "!pip install wandb -Uq\n",
    "import wandb\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "505068e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-12T23:02:25.532768Z",
     "iopub.status.busy": "2024-04-12T23:02:25.531928Z",
     "iopub.status.idle": "2024-04-12T23:02:25.618190Z",
     "shell.execute_reply": "2024-04-12T23:02:25.617055Z"
    },
    "id": "ODX4aHq7C0X0",
    "papermill": {
     "duration": 0.098064,
     "end_time": "2024-04-12T23:02:25.620067",
     "exception": false,
     "start_time": "2024-04-12T23:02:25.522003",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/deep-learning-mini-project-spring-24-nyu/cifar_test_nolabels.pkl\n",
      "/kaggle/input/deep-learning-mini-project-spring-24-nyu/cifar-10-python/cifar-10-batches-py/data_batch_1\n",
      "/kaggle/input/deep-learning-mini-project-spring-24-nyu/cifar-10-python/cifar-10-batches-py/data_batch_2\n",
      "/kaggle/input/deep-learning-mini-project-spring-24-nyu/cifar-10-python/cifar-10-batches-py/batches.meta\n",
      "/kaggle/input/deep-learning-mini-project-spring-24-nyu/cifar-10-python/cifar-10-batches-py/test_batch\n",
      "/kaggle/input/deep-learning-mini-project-spring-24-nyu/cifar-10-python/cifar-10-batches-py/data_batch_3\n",
      "/kaggle/input/deep-learning-mini-project-spring-24-nyu/cifar-10-python/cifar-10-batches-py/data_batch_5\n",
      "/kaggle/input/deep-learning-mini-project-spring-24-nyu/cifar-10-python/cifar-10-batches-py/data_batch_4\n",
      "/kaggle/input/deep-learning-mini-project-spring-24-nyu/cifar-10-python/cifar-10-batches-py/readme.html\n"
     ]
    }
   ],
   "source": [
    "if kaggle==1:\n",
    "  for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "      for filename in filenames:\n",
    "          print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df175da9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-12T23:02:25.640327Z",
     "iopub.status.busy": "2024-04-12T23:02:25.640052Z",
     "iopub.status.idle": "2024-04-12T23:02:25.665043Z",
     "shell.execute_reply": "2024-04-12T23:02:25.664227Z"
    },
    "id": "BzWO6VSrCekp",
    "papermill": {
     "duration": 0.03714,
     "end_time": "2024-04-12T23:02:25.666836",
     "exception": false,
     "start_time": "2024-04-12T23:02:25.629696",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#ResNet model\n",
    "class BasicBlock(nn.Module):\n",
    "   expansion = 1\n",
    "\n",
    "   def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "                in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                                                   stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "                self.shortcut = nn.Sequential(\n",
    "                        nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                                          kernel_size=1, stride=stride, bias=False),\n",
    "                        nn.BatchNorm2d(self.expansion*planes)\n",
    "                )\n",
    "\n",
    "   def forward(self, x):\n",
    "\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet3(nn.Module):\n",
    "   def __init__(self, block, num_blocks, channels = [64, 128, 256], num_classes=10):\n",
    "        super(ResNet3, self).__init__()\n",
    "        self.in_planes = channels[0]\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, channels[0], kernel_size=3,\n",
    "                                                   stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(channels[0])\n",
    "        self.layer1 = self._make_layer(block, channels[0], num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, channels[1], num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, channels[2], num_blocks[2], stride=2)\n",
    "        # self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(channels[2]*block.expansion, num_classes)\n",
    "\n",
    "   def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "                layers.append(block(self.in_planes, planes, stride))\n",
    "                self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "   def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        # out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 8) #changed from 4 to 8\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "class ResNet4(nn.Module):\n",
    "   def __init__(self, block, num_blocks, channels = [64, 128, 256, 512], num_classes=10):\n",
    "        super(ResNet4, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, channels[0], kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(channels[0])\n",
    "        self.layer1 = self._make_layer(block, channels[0], num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, channels[1], num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, channels[2], num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, channels[3], num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(channels[3]*block.expansion, num_classes)\n",
    "\n",
    "   def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "                layers.append(block(self.in_planes, planes, stride))\n",
    "                self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "   def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "def ResNetCustom(blocks, channels):\n",
    "  layers = len(blocks)\n",
    "  if(layers==3):\n",
    "    return ResNet3(BasicBlock, blocks, channels)\n",
    "  else:\n",
    "    return ResNet4(BasicBlock, blocks, channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "209ded3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-12T23:02:25.686283Z",
     "iopub.status.busy": "2024-04-12T23:02:25.686000Z",
     "iopub.status.idle": "2024-04-12T23:02:25.690644Z",
     "shell.execute_reply": "2024-04-12T23:02:25.689810Z"
    },
    "id": "GD2gQ6w2Cekp",
    "papermill": {
     "duration": 0.016385,
     "end_time": "2024-04-12T23:02:25.692580",
     "exception": false,
     "start_time": "2024-04-12T23:02:25.676195",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getParameters(resnet):\n",
    "  total_params = 0\n",
    "  for x in filter(lambda p: p.requires_grad, resnet.parameters()):\n",
    "    total_params += np.prod(x.data.cpu().numpy().shape)\n",
    "  print(\"Total number of params\", total_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdd8492",
   "metadata": {
    "id": "a2JYjt43Cekp",
    "papermill": {
     "duration": 0.00893,
     "end_time": "2024-04-12T23:02:25.711781",
     "exception": false,
     "start_time": "2024-04-12T23:02:25.702851",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Creating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f251f14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-12T23:02:25.731795Z",
     "iopub.status.busy": "2024-04-12T23:02:25.730994Z",
     "iopub.status.idle": "2024-04-12T23:02:25.735149Z",
     "shell.execute_reply": "2024-04-12T23:02:25.734303Z"
    },
    "id": "SSPArmBqDFdp",
    "papermill": {
     "duration": 0.015954,
     "end_time": "2024-04-12T23:02:25.736921",
     "exception": false,
     "start_time": "2024-04-12T23:02:25.720967",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96f3eb67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-12T23:02:25.756440Z",
     "iopub.status.busy": "2024-04-12T23:02:25.756198Z",
     "iopub.status.idle": "2024-04-12T23:02:26.075283Z",
     "shell.execute_reply": "2024-04-12T23:02:26.074282Z"
    },
    "id": "_cvLEj3NCekr",
    "outputId": "759736e6-0d3c-4ef9-8e08-6406ceb01fda",
    "papermill": {
     "duration": 0.331367,
     "end_time": "2024-04-12T23:02:26.077558",
     "exception": false,
     "start_time": "2024-04-12T23:02:25.746191",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of params 4918602\n"
     ]
    }
   ],
   "source": [
    "def build_model():\n",
    "    return ResNetCustom(blocks, channels)\n",
    "net = build_model()\n",
    "getParameters(net)\n",
    "if device == 'cuda':\n",
    "   net = torch.nn.DataParallel(net)\n",
    "   cudnn.benchmark = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88feaab8",
   "metadata": {
    "id": "4InP5DUSCekr",
    "papermill": {
     "duration": 0.009317,
     "end_time": "2024-04-12T23:02:26.096714",
     "exception": false,
     "start_time": "2024-04-12T23:02:26.087397",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Select Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21a4c4d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-12T23:02:26.117261Z",
     "iopub.status.busy": "2024-04-12T23:02:26.116542Z",
     "iopub.status.idle": "2024-04-12T23:02:26.120677Z",
     "shell.execute_reply": "2024-04-12T23:02:26.119846Z"
    },
    "id": "ZCn7ph6FCeks",
    "papermill": {
     "duration": 0.016586,
     "end_time": "2024-04-12T23:02:26.122684",
     "exception": false,
     "start_time": "2024-04-12T23:02:26.106098",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = optim.SGD(net.parameters(), lr=args.lr, momentum=0.9, weight_decay=5e-4)\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=t_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66259f36",
   "metadata": {
    "id": "opIF00p7Darz",
    "papermill": {
     "duration": 0.00916,
     "end_time": "2024-04-12T23:02:26.141207",
     "exception": false,
     "start_time": "2024-04-12T23:02:26.132047",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "790d5145",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-12T23:02:26.161917Z",
     "iopub.status.busy": "2024-04-12T23:02:26.161654Z",
     "iopub.status.idle": "2024-04-12T23:02:26.172341Z",
     "shell.execute_reply": "2024-04-12T23:02:26.171526Z"
    },
    "id": "BeS6L5SLDi0k",
    "papermill": {
     "duration": 0.022859,
     "end_time": "2024-04-12T23:02:26.174387",
     "exception": false,
     "start_time": "2024-04-12T23:02:26.151528",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CIFAR10Dataset(Dataset):\n",
    "    def __init__(self, data, labels, transform=None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # The data is originally stored as a numpy array with shape (num_samples, 3, 32, 32)\n",
    "        # We need to transpose it to (num_samples, 32, 32, 3) for PIL\n",
    "        image = self.data[idx].transpose((1, 2, 0))\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Convert the numpy array to a PIL Image\n",
    "        image = Image.fromarray(image.astype('uint8'))\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "def load_cifar10_batch(file):\n",
    "      with open(file, 'rb') as fo:\n",
    "          batch = pickle.load(fo, encoding='latin1')\n",
    "      data = batch['data']\n",
    "      labels = batch['labels']\n",
    "      data = data.reshape(-1, 3, 32, 32)\n",
    "      return data, labels\n",
    "\n",
    "  # Function to load all CIFAR-10 data\n",
    "def load_cifar10_data(data_dir):\n",
    "      train_data = []\n",
    "      train_labels = []\n",
    "      for i in range(1, 6):\n",
    "          batch_data, batch_labels = load_cifar10_batch(os.path.join(data_dir, f'data_batch_{i}'))\n",
    "          train_data.append(batch_data)\n",
    "          train_labels.extend(batch_labels)\n",
    "\n",
    "      train_data = np.vstack(train_data)\n",
    "      train_labels = np.array(train_labels)\n",
    "\n",
    "      test_data, test_labels = load_cifar10_batch(os.path.join(data_dir, 'test_batch'))\n",
    "      test_data = test_data.reshape(-1, 3, 32, 32)\n",
    "      test_labels = np.array(test_labels)\n",
    "\n",
    "      return train_data, train_labels, test_data, test_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "706e3d59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-12T23:02:26.194891Z",
     "iopub.status.busy": "2024-04-12T23:02:26.194620Z",
     "iopub.status.idle": "2024-04-12T23:02:26.198577Z",
     "shell.execute_reply": "2024-04-12T23:02:26.197783Z"
    },
    "id": "FYxPC2kYCeks",
    "papermill": {
     "duration": 0.016645,
     "end_time": "2024-04-12T23:02:26.200475",
     "exception": false,
     "start_time": "2024-04-12T23:02:26.183830",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir = ''\n",
    "if kaggle==1:\n",
    "  data_dir = '/kaggle/input/deep-learning-mini-project-spring-24-nyu/cifar-10-python/cifar-10-batches-py'\n",
    "else:\n",
    "  data_dir = 'cifar-10-batches-py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d0955ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-12T23:02:26.220604Z",
     "iopub.status.busy": "2024-04-12T23:02:26.220345Z",
     "iopub.status.idle": "2024-04-12T23:02:26.225928Z",
     "shell.execute_reply": "2024-04-12T23:02:26.225119Z"
    },
    "id": "a2mx8lrpCekt",
    "papermill": {
     "duration": 0.017741,
     "end_time": "2024-04-12T23:02:26.227869",
     "exception": false,
     "start_time": "2024-04-12T23:02:26.210128",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#ORIGINAL TRANSFORM\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10), #newly added\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4e62d9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-12T23:02:26.248305Z",
     "iopub.status.busy": "2024-04-12T23:02:26.248036Z",
     "iopub.status.idle": "2024-04-12T23:02:26.258234Z",
     "shell.execute_reply": "2024-04-12T23:02:26.257363Z"
    },
    "id": "yLSs1l74jc0m",
    "papermill": {
     "duration": 0.022476,
     "end_time": "2024-04-12T23:02:26.260155",
     "exception": false,
     "start_time": "2024-04-12T23:02:26.237679",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#CUTOUT\n",
    "\n",
    "class Cutout(object):\n",
    "    def __init__(self, n_holes, length):\n",
    "        self.n_holes = n_holes\n",
    "        self.length = length\n",
    "\n",
    "    def __call__(self, img):\n",
    "        h = img.size(1)\n",
    "        w = img.size(2)\n",
    "        mask = torch.ones(h, w, dtype=torch.float32)\n",
    "\n",
    "        for _ in range(self.n_holes):\n",
    "            y = torch.randint(0, h, (1,))\n",
    "            x = torch.randint(0, w, (1,))\n",
    "\n",
    "            y1 = int(torch.clamp(y - self.length / 2, 0, h).item())\n",
    "            x1 = int(torch.clamp(x - self.length / 2, 0, w).item())\n",
    "\n",
    "            y2 = int(torch.clamp(y + self.length / 2, 0, h).item())\n",
    "            x2 = int(torch.clamp(x + self.length / 2, 0, w).item())\n",
    "\n",
    "            mask[y1: y2, x1: x2] = 0\n",
    "\n",
    "        img = img * mask.unsqueeze(0)\n",
    "        return img\n",
    "\n",
    "transform_train_cutout = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10), # newly added\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    Cutout(n_holes=1, length=8)  # Cutout added\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b0f0f05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-12T23:02:26.280794Z",
     "iopub.status.busy": "2024-04-12T23:02:26.280563Z",
     "iopub.status.idle": "2024-04-12T23:02:26.286294Z",
     "shell.execute_reply": "2024-04-12T23:02:26.285480Z"
    },
    "id": "gbwCdehykXzg",
    "papermill": {
     "duration": 0.018404,
     "end_time": "2024-04-12T23:02:26.288200",
     "exception": false,
     "start_time": "2024-04-12T23:02:26.269796",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#MIXUP\n",
    "\n",
    "def mixup_data(x, y, alpha=0.2, device='cuda'):\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size).to(device)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    mixed_y = (y, y[index], lam)\n",
    "    return mixed_x, mixed_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c6ff03b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-12T23:02:26.308694Z",
     "iopub.status.busy": "2024-04-12T23:02:26.308450Z",
     "iopub.status.idle": "2024-04-12T23:02:26.317468Z",
     "shell.execute_reply": "2024-04-12T23:02:26.316576Z"
    },
    "id": "JPjDps3Bkmq7",
    "papermill": {
     "duration": 0.02154,
     "end_time": "2024-04-12T23:02:26.319412",
     "exception": false,
     "start_time": "2024-04-12T23:02:26.297872",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#CUTMIX\n",
    "\n",
    "def cutmix_data(inputs, targets, alpha=1.0):\n",
    "    # Generate mixed sample\n",
    "    lam = np.random.beta(alpha, alpha) if alpha > 0 else 1\n",
    "    batch_size = inputs.size(0)\n",
    "    index = torch.randperm(batch_size).to(inputs.device)\n",
    "\n",
    "    # Random rectangle region\n",
    "    H, W = inputs.size(2), inputs.size(3)\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = int(W * cut_rat)\n",
    "    cut_h = int(H * cut_rat)\n",
    "\n",
    "    # Uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    inputs[:, :, bby1:bby2, bbx1:bbx2] = inputs[index, :, bby1:bby2, bbx1:bbx2]\n",
    "    # Adjust lambda to exactly match the pixel ratio\n",
    "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (inputs.size()[-1] * inputs.size()[-2]))\n",
    "    targets_a, targets_b = targets, targets[index]\n",
    "\n",
    "    return inputs, targets_a, targets_b, lam\n",
    "\n",
    "# def rand_bbox(size, lam):\n",
    "#     W = size[2]\n",
    "#     H = size[3]\n",
    "#     cut_rat = np.sqrt(1. - lam)\n",
    "#     cut_w = int(W * cut_rat)\n",
    "#     cut_h = int(H * cut_rat)\n",
    "\n",
    "#     # uniform\n",
    "#     cx = np.random.randint(W)\n",
    "#     cy = np.random.randint(H)\n",
    "\n",
    "#     bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "#     bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "#     bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "#     bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "#     return bbx1, bby1, bbx2, bby2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe6f918",
   "metadata": {
    "id": "uyedtmOnkYLd",
    "papermill": {
     "duration": 0.009532,
     "end_time": "2024-04-12T23:02:26.339063",
     "exception": false,
     "start_time": "2024-04-12T23:02:26.329531",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Create dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef4d5480",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-12T23:02:26.359909Z",
     "iopub.status.busy": "2024-04-12T23:02:26.359376Z",
     "iopub.status.idle": "2024-04-12T23:02:28.231398Z",
     "shell.execute_reply": "2024-04-12T23:02:28.230506Z"
    },
    "id": "cwVmAP6djunF",
    "outputId": "542a587a-bc18-4733-b8ad-bd3084d33046",
    "papermill": {
     "duration": 1.884911,
     "end_time": "2024-04-12T23:02:28.233607",
     "exception": false,
     "start_time": "2024-04-12T23:02:26.348696",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if load_dataset_online==0:\n",
    "  train_data, train_labels, test_data, test_labels = load_cifar10_data(data_dir)\n",
    "  train_dataset = CIFAR10Dataset(train_data, train_labels, transform=transform_train)\n",
    "  test_dataset = CIFAR10Dataset(test_data, test_labels, transform=transform_test)\n",
    "\n",
    "  train_dataset_cutout = CIFAR10Dataset(train_data, train_labels, transform=transform_train_cutout)\n",
    "else:\n",
    "  train_dataset = torchvision.datasets.CIFAR10(root='/data', train=True, download=True, transform=transform_train)\n",
    "  test_dataset = torchvision.datasets.CIFAR10(root='/data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "  train_dataset_cutout = torchvision.datasets.CIFAR10(root='/datacutout', train=True, download=True, transform=transform_train_cutout)\n",
    "\n",
    "# Create DataLoaders\n",
    "trainloader = DataLoader(train_dataset, batch_size=our_batch_size, shuffle=True, num_workers=our_number_workers)\n",
    "testloader = DataLoader(test_dataset, batch_size=our_batch_size, shuffle=False, num_workers=our_number_workers)\n",
    "\n",
    "trainloader_cutout = DataLoader(train_dataset_cutout, batch_size=our_batch_size, shuffle=True, num_workers=our_number_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ca6bedf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-12T23:02:28.254804Z",
     "iopub.status.busy": "2024-04-12T23:02:28.254505Z",
     "iopub.status.idle": "2024-04-12T23:02:28.259518Z",
     "shell.execute_reply": "2024-04-12T23:02:28.258677Z"
    },
    "id": "5Yfjcn2BCekt",
    "outputId": "7c61c0b7-e4bb-4a48-cc2b-df229b59c7d4",
    "papermill": {
     "duration": 0.017685,
     "end_time": "2024-04-12T23:02:28.261453",
     "exception": false,
     "start_time": "2024-04-12T23:02:28.243768",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391\n",
      "391\n",
      "79\n"
     ]
    }
   ],
   "source": [
    "print(len(trainloader))\n",
    "print(len(trainloader_cutout))\n",
    "print(len(testloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d602cafe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-12T23:02:28.282624Z",
     "iopub.status.busy": "2024-04-12T23:02:28.282372Z",
     "iopub.status.idle": "2024-04-12T23:02:28.288277Z",
     "shell.execute_reply": "2024-04-12T23:02:28.287381Z"
    },
    "id": "-shVuMQxCeku",
    "papermill": {
     "duration": 0.018704,
     "end_time": "2024-04-12T23:02:28.290137",
     "exception": false,
     "start_time": "2024-04-12T23:02:28.271433",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def adaptive_gradient_clipping(parameters, clip_factor=0.01, eps=1e-3):\n",
    "    with torch.no_grad():\n",
    "        for p in parameters:\n",
    "            if p.grad is not None:\n",
    "                param_norm = torch.norm(p.data, p=2)\n",
    "                grad_norm = torch.norm(p.grad.data, p=2)\n",
    "                max_norm = param_norm * clip_factor\n",
    "                if grad_norm > max_norm + eps:\n",
    "                    clip_coef = max_norm / (grad_norm + eps)\n",
    "                    p.grad.data.mul_(clip_coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e10a033b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-12T23:02:28.311155Z",
     "iopub.status.busy": "2024-04-12T23:02:28.310899Z",
     "iopub.status.idle": "2024-04-12T23:02:28.314762Z",
     "shell.execute_reply": "2024-04-12T23:02:28.313948Z"
    },
    "id": "A1oYs3F3v-xO",
    "papermill": {
     "duration": 0.016524,
     "end_time": "2024-04-12T23:02:28.316631",
     "exception": false,
     "start_time": "2024-04-12T23:02:28.300107",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# do_clipping = 0\n",
    "# if args_optimizer=='sgdn':\n",
    "#   optimizer = optim.SGD(net.parameters(), lr=args_lr, momentum=0.9, weight_decay=args_weight_decay, nesterov=True)\n",
    "# elif args_optimizer=='adadelta':\n",
    "#   optimizer = optim.Adadelta(net.parameters(), lr=args_lr, weight_decay=args_weight_decay)\n",
    "# elif args_optimizer=='adadelta-clipping':\n",
    "#   optimizer = optim.Adadelta(net.parameters(), lr=args_lr, weight_decay=args_weight_decay)\n",
    "#   do_clipping = 1\n",
    "# else: #sgd\n",
    "#   optimizer = optim.SGD(net.parameters(), lr=args_lr, momentum=0.9, weight_decay=args_weight_decay)\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=t_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c12dc1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-12T23:02:28.338134Z",
     "iopub.status.busy": "2024-04-12T23:02:28.337833Z",
     "iopub.status.idle": "2024-04-12T23:02:28.360262Z",
     "shell.execute_reply": "2024-04-12T23:02:28.359374Z"
    },
    "id": "lNqJRjCWCeku",
    "papermill": {
     "duration": 0.035545,
     "end_time": "2024-04-12T23:02:28.362255",
     "exception": false,
     "start_time": "2024-04-12T23:02:28.326710",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(epoch, optimizer, mynet, mytrainloader, mixup = 0, cutmix = 0, clipping=0):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    epoch_start = perf_counter()  # Start timing the epoch\n",
    "    mynet.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    dataloading_time = 0\n",
    "    training_time = 0\n",
    "\n",
    "    train_accuracy = []\n",
    "    train_losses = []\n",
    "#   print(\"starting loop\")\n",
    "    #try:\n",
    "    for batch_idx, (inputs, targets) in enumerate(mytrainloader):\n",
    "\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        if mixup==1:\n",
    "          inputs, mixed_targets = mixup_data(inputs, targets, alpha=0.5, device=device)\n",
    "          targets, shuffled_targets, lam = mixed_targets\n",
    "\n",
    "          optimizer.zero_grad()\n",
    "          outputs = mynet(inputs)\n",
    "\n",
    "          loss = lam * criterion(outputs, targets) + (1 - lam) * criterion(outputs, shuffled_targets)\n",
    "          loss.backward()\n",
    "          if (clipping==1):\n",
    "              adaptive_gradient_clipping(mynet.parameters(), clip_factor=0.01)\n",
    "          optimizer.step()\n",
    "\n",
    "          _, predicted = outputs.max(1)\n",
    "          correct += (predicted.eq(targets) | predicted.eq(shuffled_targets)).sum().item()\n",
    "          total += targets.size(0)\n",
    "          train_loss += loss.item()\n",
    "          epoch_accuracy = 100. * correct / total\n",
    "\n",
    "          train_accuracy.append(epoch_accuracy)\n",
    "          train_losses.append(loss.item())\n",
    "\n",
    "        elif cutmix==1:\n",
    "\n",
    "          targets_a, targets_b = targets.clone(), targets.clone()\n",
    "          if np.random.rand() < 0.5:\n",
    "            inputs, targets_a, targets_b, lam = cutmix_data(inputs, targets, alpha=1.0)\n",
    "            outputs = mynet(inputs)\n",
    "            loss = lam * criterion(outputs, targets_a) + (1 - lam) * criterion(outputs, targets_b)\n",
    "          else:\n",
    "            outputs = mynet(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "          optimizer.zero_grad()\n",
    "          loss.backward()\n",
    "          if (clipping==1):\n",
    "              adaptive_gradient_clipping(mynet.parameters(), clip_factor=0.01)\n",
    "          optimizer.step()\n",
    "\n",
    "          _, predicted = outputs.max(1)\n",
    "          correct += (predicted.eq(targets_a) | predicted.eq(targets_b)).sum().item()\n",
    "          total += targets.size(0)\n",
    "          train_loss += loss.item()\n",
    "          epoch_accuracy = 100. * correct / total\n",
    "\n",
    "          train_accuracy.append(epoch_accuracy)\n",
    "          train_losses.append(loss.item())\n",
    "\n",
    "        else:\n",
    "\n",
    "          optimizer.zero_grad()\n",
    "          outputs = mynet(inputs)\n",
    "\n",
    "          loss = criterion(outputs, targets)\n",
    "          loss.backward()\n",
    "          if (clipping==1):\n",
    "              adaptive_gradient_clipping(mynet.parameters(), clip_factor=0.01)\n",
    "          optimizer.step()\n",
    "\n",
    "          train_loss += loss.item()\n",
    "          _, predicted = outputs.max(1)\n",
    "          total += targets.size(0)\n",
    "          correct += predicted.eq(targets).sum().item()\n",
    "          epoch_accuracy = 100. * correct / total\n",
    "\n",
    "          train_accuracy.append(epoch_accuracy)\n",
    "          train_losses.append(loss.item())\n",
    "\n",
    "\n",
    "    epoch_loss = train_loss / len(mytrainloader)\n",
    "\n",
    "    epoch_accuracy = 100. * correct / total\n",
    "    print(f'\\nEpoch: {epoch}, Loss: {epoch_loss:.3f}, Acc: {epoch_accuracy:.3f}%')\n",
    "    # print('Data loading time:', dataloading_time)\n",
    "    # print('Training time:', training_time)\n",
    "    # print('Total epoch time:', total_epoch_time)\n",
    "\n",
    "    return epoch_accuracy, epoch_loss\n",
    "\n",
    "\n",
    "def test(optimizer, mynet):\n",
    "    mynet.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    test_accuracy = []\n",
    "    test_losses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = mynet(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            batch_loss = loss.item()\n",
    "            test_loss += batch_loss\n",
    "            _, predicted = outputs.max(1)\n",
    "            batch_sizee = targets.size(0)\n",
    "            total += batch_sizee\n",
    "            correct_batch =  predicted.eq(targets).sum().item()\n",
    "            correct += correct_batch\n",
    "\n",
    "            # Compute and store epoch-wise test accuracy\n",
    "            epoch_accuracy = 100. * correct / total\n",
    "            test_accuracy.append(epoch_accuracy)\n",
    "            test_losses.append(loss.item())\n",
    "\n",
    "            batch_accuracy = 100. * correct_batch / batch_sizee\n",
    "\n",
    "    epoch_loss = test_loss / len(testloader)\n",
    "    epoch_accuracy = 100. * correct / total\n",
    "    print(f'\\nTest Loss: {epoch_loss:.3f}, Acc: {epoch_accuracy:.3f}%')\n",
    "\n",
    "    return epoch_accuracy, epoch_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f2eda5d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-12T23:02:28.383733Z",
     "iopub.status.busy": "2024-04-12T23:02:28.383490Z",
     "iopub.status.idle": "2024-04-12T23:02:28.388977Z",
     "shell.execute_reply": "2024-04-12T23:02:28.388180Z"
    },
    "id": "CkvQ6kFACeku",
    "papermill": {
     "duration": 0.018281,
     "end_time": "2024-04-12T23:02:28.390809",
     "exception": false,
     "start_time": "2024-04-12T23:02:28.372528",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Saving Model\n",
    "if kaggle==1:\n",
    "  output_directory = '/kaggle/working/'\n",
    "else:\n",
    "  output_directory = ''\n",
    "import time\n",
    "def saveModel(mynet, number_of_epochs):\n",
    "    timestr = time.strftime(\"%m%d-%H%M\")\n",
    "    directory = os.path.join(output_directory, str(number_of_epochs))\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    torch.save({\n",
    "        'net': mynet,\n",
    "    }, os.path.join(directory, '{}_{}.tar'.format('Time', timestr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cdc29de3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-12T23:02:28.412440Z",
     "iopub.status.busy": "2024-04-12T23:02:28.411882Z",
     "iopub.status.idle": "2024-04-12T23:02:28.419026Z",
     "shell.execute_reply": "2024-04-12T23:02:28.418160Z"
    },
    "id": "8UQPJT7JCeku",
    "papermill": {
     "duration": 0.019768,
     "end_time": "2024-04-12T23:02:28.420831",
     "exception": false,
     "start_time": "2024-04-12T23:02:28.401063",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plotting function for train vs test graph\n",
    "\n",
    "def plot_train_test_accuracy(train_acc, test_acc, train_loss, test_loss):\n",
    "    epochs = range(1, len(train_acc) + 1)\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Plotting accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_acc, 'b', label='Training Accuracy')\n",
    "    plt.plot(epochs, test_acc, 'r', label='Test Accuracy')\n",
    "    plt.title('Training and Test Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plotting loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_loss, 'b', label='Training Loss')\n",
    "    plt.plot(epochs, test_loss, 'r', label='Test Loss')\n",
    "    plt.title('Training and Test Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1a82061f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-12T23:02:28.442663Z",
     "iopub.status.busy": "2024-04-12T23:02:28.442049Z",
     "iopub.status.idle": "2024-04-12T23:02:28.447997Z",
     "shell.execute_reply": "2024-04-12T23:02:28.447259Z"
    },
    "id": "vsnUOSRoCeku",
    "papermill": {
     "duration": 0.018768,
     "end_time": "2024-04-12T23:02:28.449917",
     "exception": false,
     "start_time": "2024-04-12T23:02:28.431149",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.max_validation_acc = float('inf')\n",
    "\n",
    "    def early_stop(self, validation_accuracy):\n",
    "        if validation_accuracy > self.max_validation_acc:\n",
    "            self.max_validation_acc = validation_accuracy\n",
    "            self.counter = 0\n",
    "        elif validation_accuracy < (self.max_validation_acc - self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb747f19",
   "metadata": {
    "id": "QPKSirneCeku",
    "papermill": {
     "duration": 0.010034,
     "end_time": "2024-04-12T23:02:28.470289",
     "exception": false,
     "start_time": "2024-04-12T23:02:28.460255",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "WANDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7af2a855",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-12T23:02:28.492193Z",
     "iopub.status.busy": "2024-04-12T23:02:28.491547Z",
     "iopub.status.idle": "2024-04-12T23:02:30.104204Z",
     "shell.execute_reply": "2024-04-12T23:02:30.103276Z"
    },
    "id": "GQEUgM_NCekv",
    "outputId": "0e17ec8c-8b2e-48e3-cdf3-ea6f6c62d11f",
    "papermill": {
     "duration": 1.625794,
     "end_time": "2024-04-12T23:02:30.106309",
     "exception": false,
     "start_time": "2024-04-12T23:02:28.480515",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    }
   ],
   "source": [
    "if use_wandb==1:\n",
    "  wandb.login(key = wandbapi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "452927ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-12T23:02:30.129162Z",
     "iopub.status.busy": "2024-04-12T23:02:30.128859Z",
     "iopub.status.idle": "2024-04-12T23:02:30.134361Z",
     "shell.execute_reply": "2024-04-12T23:02:30.133488Z"
    },
    "id": "x6kcDf4OCekv",
    "papermill": {
     "duration": 0.018895,
     "end_time": "2024-04-12T23:02:30.136267",
     "exception": false,
     "start_time": "2024-04-12T23:02:30.117372",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sweep_config = {\n",
    "#     'method': 'random'\n",
    "#     }\n",
    "# metric = {\n",
    "#     'name': 'loss',\n",
    "#     'goal': 'minimize'\n",
    "#     }\n",
    "# sweep_config['metric'] = metric\n",
    "# parameters_dict = {\n",
    "#     'optimizer': {\n",
    "#         'values': ['adadelta', 'sgd', 'adadelta-clipping']\n",
    "#         },\n",
    "#     'learning_rate': {\n",
    "#           'values': [0.1, 0.01]\n",
    "#         }\n",
    "#     }\n",
    "\n",
    "# sweep_config['parameters'] = parameters_dict\n",
    "\n",
    "\n",
    "sweep_config = {\n",
    "    'method': 'grid'\n",
    "    }\n",
    "metric = {\n",
    "    'name': 'loss',\n",
    "    'goal': 'minimize'\n",
    "    }\n",
    "sweep_config['metric'] = metric\n",
    "parameters_dict = {\n",
    "    'dataaug': {\n",
    "        #'values': ['cutout', 'mixup', 'cutmix', 'cutout-mixup']\n",
    "        'values': ['cutout']\n",
    "        # 'values': [ 'mixup', 'cutmix', 'cutout-mixup']\n",
    "        }\n",
    "    }\n",
    "\n",
    "sweep_config['parameters'] = parameters_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4c6d76ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-12T23:02:30.159132Z",
     "iopub.status.busy": "2024-04-12T23:02:30.158833Z",
     "iopub.status.idle": "2024-04-12T23:02:30.163521Z",
     "shell.execute_reply": "2024-04-12T23:02:30.162744Z"
    },
    "id": "A9TTBRV8Cekv",
    "outputId": "55e4772e-2557-45bc-c827-10daf16bfe43",
    "papermill": {
     "duration": 0.018428,
     "end_time": "2024-04-12T23:02:30.165432",
     "exception": false,
     "start_time": "2024-04-12T23:02:30.147004",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'method': 'grid',\n",
      " 'metric': {'goal': 'minimize', 'name': 'loss'},\n",
      " 'parameters': {'dataaug': {'values': ['cutout']}}}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(sweep_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bc38a57a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-12T23:02:30.188346Z",
     "iopub.status.busy": "2024-04-12T23:02:30.188087Z",
     "iopub.status.idle": "2024-04-12T23:02:30.203880Z",
     "shell.execute_reply": "2024-04-12T23:02:30.203254Z"
    },
    "id": "utgKnD1eCekv",
    "papermill": {
     "duration": 0.029522,
     "end_time": "2024-04-12T23:02:30.205702",
     "exception": false,
     "start_time": "2024-04-12T23:02:30.176180",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "def trainwandb(config=None):\n",
    "    wandb.init(config=config)\n",
    "    config = wandb.config\n",
    "\n",
    "    net = build_model()\n",
    "    getParameters(net)\n",
    "    if device == 'cuda':\n",
    "       net = torch.nn.DataParallel(net)\n",
    "       cudnn.benchmark = True\n",
    "\n",
    "    # do_clipping = 0\n",
    "    # if config.optimizer == 'adadelta':\n",
    "    #     optimizer = optim.Adadelta(net.parameters(), lr=config.learning_rate, weight_decay=0.0005)\n",
    "    # elif config.optimizer == 'adadelta-clipping':\n",
    "    #     optimizer = optim.Adadelta(net.parameters(), lr=config.learning_rate, weight_decay=0.0005)\n",
    "    #     do_clipping = 1\n",
    "    # elif config.optimizer == 'sgd':\n",
    "    #     optimizer = optim.SGD(net.parameters(), lr=config.learning_rate, momentum=0.9, weight_decay=0.0005)\n",
    "    # else: #sgdn\n",
    "    #     optimizer = optim.SGD(net.parameters(), lr=config.learning_rate, momentum=0.9, weight_decay=0.0005, nesterov=True)\n",
    "\n",
    "    # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=t_max)\n",
    "\n",
    "    args_lr=0.01\n",
    "    \n",
    "    do_clipping = 0\n",
    "    if args_optimizer=='sgdn':\n",
    "      optimizer = optim.SGD(net.parameters(), lr=args_lr, momentum=0.9, weight_decay=args_weight_decay, nesterov=True)\n",
    "    elif args_optimizer=='adadelta':\n",
    "      optimizer = optim.Adadelta(net.parameters(), lr=args_lr, weight_decay=args_weight_decay)\n",
    "    elif args_optimizer=='adadelta-clipping':\n",
    "      optimizer = optim.Adadelta(net.parameters(), lr=args_lr, weight_decay=args_weight_decay)\n",
    "      do_clipping = 1\n",
    "    else: #sgd\n",
    "      optimizer = optim.SGD(net.parameters(), lr=args_lr, momentum=0.9, weight_decay=args_weight_decay)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=t_max)\n",
    "\n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "    if earlystop:\n",
    "      early_stopper = EarlyStopper(patience=5, min_delta=2)\n",
    "\n",
    "    for epoch in range(start_epoch, start_epoch+number_of_epochs):\n",
    "        if config.dataaug=='cutout': #'cutout', 'mixup', 'cutmix', 'cutout-mixup' #trainloader, trainloader_cutout\n",
    "          train_acc, train_loss = train(epoch, optimizer, net, trainloader_cutout, 0, 0, do_clipping)\n",
    "        elif config.dataaug=='mixup':\n",
    "          train_acc, train_loss = train(epoch, optimizer, net, trainloader, 1, 0, do_clipping)\n",
    "        elif config.dataaug=='cutmix':\n",
    "          train_acc, train_loss = train(epoch, optimizer, net, trainloader, 0, 1, do_clipping)\n",
    "        else: #cutout-mixup\n",
    "          train_acc, train_loss = train(epoch, optimizer, net, trainloader_cutout, 1, 0, do_clipping)\n",
    "\n",
    "        test_acc, test_loss = test(optimizer, net)\n",
    "\n",
    "        wandb.log({\"epoch\": epoch, \"train_accuracy\": train_acc, \"train_loss\": train_loss,\n",
    "                   \"test_accuracy\": test_acc, \"test_loss\": test_loss})\n",
    "\n",
    "        scheduler.step()\n",
    "        \n",
    "        if epoch+start_epoch==80: #80\n",
    "            args_lr/=10\n",
    "        if epoch+start_epoch==150: #150\n",
    "            args_lr/=10\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr']=args_lr\n",
    "        \n",
    "        train_accuracies.append(train_acc)\n",
    "        test_accuracies.append(test_acc)\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "        if(epoch%save_freq==0):\n",
    "            saveModel(net, epoch)\n",
    "            #test()\n",
    "        if earlystop:\n",
    "          if early_stopper.early_stop(test_acc) and test_acc>earlystop_threshold:\n",
    "            wandb.log({\"early_stop\": epoch})\n",
    "            break\n",
    "\n",
    "    saveModel(net, number_of_epochs)\n",
    "\n",
    "    # Plot train vs test accuracy graph\n",
    "    plot_train_test_accuracy(train_accuracies, test_accuracies, train_losses, test_losses)\n",
    "\n",
    "    test(optimizer, net)\n",
    "    wandb.log({\"final_test_loss\": test_losses[-1]})\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fcf4dd63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-12T23:02:30.228562Z",
     "iopub.status.busy": "2024-04-12T23:02:30.228306Z",
     "iopub.status.idle": "2024-04-12T23:02:30.591302Z",
     "shell.execute_reply": "2024-04-12T23:02:30.590213Z"
    },
    "id": "TqgeP9C8Cekv",
    "outputId": "ba7fccbc-9209-4349-a026-7e08b85001c7",
    "papermill": {
     "duration": 0.376795,
     "end_time": "2024-04-12T23:02:30.593336",
     "exception": false,
     "start_time": "2024-04-12T23:02:30.216541",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 7rbk9m9t\n",
      "Sweep URL: https://wandb.ai/kavyagupta/dl-mini-353-cutout/sweeps/7rbk9m9t\n"
     ]
    }
   ],
   "source": [
    "if use_wandb==1:\n",
    "  sweep_id = wandb.sweep(sweep_config, project=\"dl-mini-353-cutout\")\n",
    "  #  sweep_id = \"classical/pytorch-sweeps-demo/mnpi1d3n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "749d588b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-12T23:02:30.616846Z",
     "iopub.status.busy": "2024-04-12T23:02:30.616558Z",
     "iopub.status.idle": "2024-04-13T00:37:57.957084Z",
     "shell.execute_reply": "2024-04-13T00:37:57.955599Z"
    },
    "id": "CCCoSuLJCekv",
    "outputId": "42da633f-fd03-4f90-8c35-09e4829a1c4a",
    "papermill": {
     "duration": 5727.355397,
     "end_time": "2024-04-13T00:37:57.960122",
     "exception": false,
     "start_time": "2024-04-12T23:02:30.604725",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ae12ytgm with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataaug: cutout\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkavyagupta\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240412_230234-ae12ytgm\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mearnest-sweep-1\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/kavyagupta/dl-mini-353-cutout\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/kavyagupta/dl-mini-353-cutout/sweeps/7rbk9m9t\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/kavyagupta/dl-mini-353-cutout/runs/ae12ytgm\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of params 4918602\n",
      "\n",
      "Epoch: 0\n",
      "\n",
      "Epoch: 0, Loss: 1.492, Acc: 44.928%\n",
      "\n",
      "Test Loss: 1.199, Acc: 58.830%\n",
      "\n",
      "Epoch: 1\n",
      "\n",
      "Epoch: 1, Loss: 1.065, Acc: 61.868%\n",
      "\n",
      "Test Loss: 1.361, Acc: 58.680%\n",
      "\n",
      "Epoch: 2\n",
      "\n",
      "Epoch: 2, Loss: 0.866, Acc: 69.288%\n",
      "\n",
      "Test Loss: 1.060, Acc: 67.120%\n",
      "\n",
      "Epoch: 3\n",
      "\n",
      "Epoch: 3, Loss: 0.749, Acc: 73.810%\n",
      "\n",
      "Test Loss: 0.756, Acc: 74.350%\n",
      "\n",
      "Epoch: 4\n",
      "\n",
      "Epoch: 4, Loss: 0.668, Acc: 76.740%\n",
      "\n",
      "Test Loss: 0.682, Acc: 77.440%\n",
      "\n",
      "Epoch: 5\n",
      "\n",
      "Epoch: 5, Loss: 0.613, Acc: 78.538%\n",
      "\n",
      "Test Loss: 0.721, Acc: 77.090%\n",
      "\n",
      "Epoch: 6\n",
      "\n",
      "Epoch: 6, Loss: 0.561, Acc: 80.316%\n",
      "\n",
      "Test Loss: 0.528, Acc: 82.550%\n",
      "\n",
      "Epoch: 7\n",
      "\n",
      "Epoch: 7, Loss: 0.520, Acc: 81.868%\n",
      "\n",
      "Test Loss: 0.738, Acc: 78.500%\n",
      "\n",
      "Epoch: 8\n",
      "\n",
      "Epoch: 8, Loss: 0.495, Acc: 82.758%\n",
      "\n",
      "Test Loss: 0.606, Acc: 80.410%\n",
      "\n",
      "Epoch: 9\n",
      "\n",
      "Epoch: 9, Loss: 0.469, Acc: 83.594%\n",
      "\n",
      "Test Loss: 0.460, Acc: 84.860%\n",
      "\n",
      "Epoch: 10\n",
      "\n",
      "Epoch: 10, Loss: 0.436, Acc: 84.768%\n",
      "\n",
      "Test Loss: 0.544, Acc: 83.070%\n",
      "\n",
      "Epoch: 11\n",
      "\n",
      "Epoch: 11, Loss: 0.412, Acc: 85.512%\n",
      "\n",
      "Test Loss: 0.469, Acc: 84.760%\n",
      "\n",
      "Epoch: 12\n",
      "\n",
      "Epoch: 12, Loss: 0.400, Acc: 85.990%\n",
      "\n",
      "Test Loss: 0.397, Acc: 86.660%\n",
      "\n",
      "Epoch: 13\n",
      "\n",
      "Epoch: 13, Loss: 0.379, Acc: 86.774%\n",
      "\n",
      "Test Loss: 0.392, Acc: 87.330%\n",
      "\n",
      "Epoch: 14\n",
      "\n",
      "Epoch: 14, Loss: 0.362, Acc: 87.358%\n",
      "\n",
      "Test Loss: 0.384, Acc: 87.780%\n",
      "\n",
      "Epoch: 15\n",
      "\n",
      "Epoch: 15, Loss: 0.354, Acc: 87.486%\n",
      "\n",
      "Test Loss: 0.408, Acc: 86.580%\n",
      "\n",
      "Epoch: 16\n",
      "\n",
      "Epoch: 16, Loss: 0.336, Acc: 88.332%\n",
      "\n",
      "Test Loss: 0.387, Acc: 86.860%\n",
      "\n",
      "Epoch: 17\n",
      "\n",
      "Epoch: 17, Loss: 0.327, Acc: 88.586%\n",
      "\n",
      "Test Loss: 0.377, Acc: 87.500%\n",
      "\n",
      "Epoch: 18\n",
      "\n",
      "Epoch: 18, Loss: 0.316, Acc: 88.974%\n",
      "\n",
      "Test Loss: 0.445, Acc: 85.890%\n",
      "\n",
      "Epoch: 19\n",
      "\n",
      "Epoch: 19, Loss: 0.304, Acc: 89.258%\n",
      "\n",
      "Test Loss: 0.377, Acc: 87.880%\n",
      "\n",
      "Epoch: 20\n",
      "\n",
      "Epoch: 20, Loss: 0.292, Acc: 89.662%\n",
      "\n",
      "Test Loss: 0.334, Acc: 89.180%\n",
      "\n",
      "Epoch: 21\n",
      "\n",
      "Epoch: 21, Loss: 0.291, Acc: 89.718%\n",
      "\n",
      "Test Loss: 0.373, Acc: 87.840%\n",
      "\n",
      "Epoch: 22\n",
      "\n",
      "Epoch: 22, Loss: 0.278, Acc: 90.292%\n",
      "\n",
      "Test Loss: 0.371, Acc: 88.100%\n",
      "\n",
      "Epoch: 23\n",
      "\n",
      "Epoch: 23, Loss: 0.272, Acc: 90.442%\n",
      "\n",
      "Test Loss: 0.366, Acc: 88.710%\n",
      "\n",
      "Epoch: 24\n",
      "\n",
      "Epoch: 24, Loss: 0.262, Acc: 90.876%\n",
      "\n",
      "Test Loss: 0.336, Acc: 89.300%\n",
      "\n",
      "Epoch: 25\n",
      "\n",
      "Epoch: 25, Loss: 0.255, Acc: 91.056%\n",
      "\n",
      "Test Loss: 0.351, Acc: 88.370%\n",
      "\n",
      "Epoch: 26\n",
      "\n",
      "Epoch: 26, Loss: 0.245, Acc: 91.466%\n",
      "\n",
      "Test Loss: 0.348, Acc: 88.670%\n",
      "\n",
      "Epoch: 27\n",
      "\n",
      "Epoch: 27, Loss: 0.245, Acc: 91.370%\n",
      "\n",
      "Test Loss: 0.357, Acc: 88.470%\n",
      "\n",
      "Epoch: 28\n",
      "\n",
      "Epoch: 28, Loss: 0.238, Acc: 91.614%\n",
      "\n",
      "Test Loss: 0.323, Acc: 89.440%\n",
      "\n",
      "Epoch: 29\n",
      "\n",
      "Epoch: 29, Loss: 0.233, Acc: 91.970%\n",
      "\n",
      "Test Loss: 0.343, Acc: 88.970%\n",
      "\n",
      "Epoch: 30\n",
      "\n",
      "Epoch: 30, Loss: 0.224, Acc: 92.206%\n",
      "\n",
      "Test Loss: 0.349, Acc: 89.020%\n",
      "\n",
      "Epoch: 31\n",
      "\n",
      "Epoch: 31, Loss: 0.221, Acc: 92.322%\n",
      "\n",
      "Test Loss: 0.301, Acc: 90.560%\n",
      "\n",
      "Epoch: 32\n",
      "\n",
      "Epoch: 32, Loss: 0.209, Acc: 92.728%\n",
      "\n",
      "Test Loss: 0.292, Acc: 90.820%\n",
      "\n",
      "Epoch: 33\n",
      "\n",
      "Epoch: 33, Loss: 0.203, Acc: 92.886%\n",
      "\n",
      "Test Loss: 0.289, Acc: 90.830%\n",
      "\n",
      "Epoch: 34\n",
      "\n",
      "Epoch: 34, Loss: 0.202, Acc: 92.920%\n",
      "\n",
      "Test Loss: 0.338, Acc: 89.750%\n",
      "\n",
      "Epoch: 35\n",
      "\n",
      "Epoch: 35, Loss: 0.199, Acc: 93.024%\n",
      "\n",
      "Test Loss: 0.293, Acc: 91.110%\n",
      "\n",
      "Epoch: 36\n",
      "\n",
      "Epoch: 36, Loss: 0.198, Acc: 93.018%\n",
      "\n",
      "Test Loss: 0.310, Acc: 90.510%\n",
      "\n",
      "Epoch: 37\n",
      "\n",
      "Epoch: 37, Loss: 0.195, Acc: 93.046%\n",
      "\n",
      "Test Loss: 0.305, Acc: 90.540%\n",
      "\n",
      "Epoch: 38\n",
      "\n",
      "Epoch: 38, Loss: 0.189, Acc: 93.410%\n",
      "\n",
      "Test Loss: 0.327, Acc: 89.690%\n",
      "\n",
      "Epoch: 39\n",
      "\n",
      "Epoch: 39, Loss: 0.185, Acc: 93.534%\n",
      "\n",
      "Test Loss: 0.358, Acc: 89.420%\n",
      "\n",
      "Epoch: 40\n",
      "\n",
      "Epoch: 40, Loss: 0.181, Acc: 93.684%\n",
      "\n",
      "Test Loss: 0.315, Acc: 90.220%\n",
      "\n",
      "Epoch: 41\n",
      "\n",
      "Epoch: 41, Loss: 0.182, Acc: 93.672%\n",
      "\n",
      "Test Loss: 0.321, Acc: 89.990%\n",
      "\n",
      "Epoch: 42\n",
      "\n",
      "Epoch: 42, Loss: 0.172, Acc: 93.966%\n",
      "\n",
      "Test Loss: 0.339, Acc: 89.710%\n",
      "\n",
      "Epoch: 43\n",
      "\n",
      "Epoch: 43, Loss: 0.176, Acc: 93.746%\n",
      "\n",
      "Test Loss: 0.299, Acc: 91.050%\n",
      "\n",
      "Epoch: 44\n",
      "\n",
      "Epoch: 44, Loss: 0.172, Acc: 93.896%\n",
      "\n",
      "Test Loss: 0.315, Acc: 90.780%\n",
      "\n",
      "Epoch: 45\n",
      "\n",
      "Epoch: 45, Loss: 0.169, Acc: 94.100%\n",
      "\n",
      "Test Loss: 0.271, Acc: 91.340%\n",
      "\n",
      "Epoch: 46\n",
      "\n",
      "Epoch: 46, Loss: 0.159, Acc: 94.460%\n",
      "\n",
      "Test Loss: 0.314, Acc: 90.470%\n",
      "\n",
      "Epoch: 47\n",
      "\n",
      "Epoch: 47, Loss: 0.164, Acc: 94.312%\n",
      "\n",
      "Test Loss: 0.300, Acc: 90.520%\n",
      "\n",
      "Epoch: 48\n",
      "\n",
      "Epoch: 48, Loss: 0.160, Acc: 94.388%\n",
      "\n",
      "Test Loss: 0.345, Acc: 89.670%\n",
      "\n",
      "Epoch: 49\n",
      "\n",
      "Epoch: 49, Loss: 0.154, Acc: 94.584%\n",
      "\n",
      "Test Loss: 0.289, Acc: 91.080%\n",
      "\n",
      "Epoch: 50\n",
      "\n",
      "Epoch: 50, Loss: 0.157, Acc: 94.520%\n",
      "\n",
      "Test Loss: 0.287, Acc: 91.520%\n",
      "\n",
      "Epoch: 51\n",
      "\n",
      "Epoch: 51, Loss: 0.154, Acc: 94.568%\n",
      "\n",
      "Test Loss: 0.302, Acc: 90.910%\n",
      "\n",
      "Epoch: 52\n",
      "\n",
      "Epoch: 52, Loss: 0.150, Acc: 94.826%\n",
      "\n",
      "Test Loss: 0.318, Acc: 90.630%\n",
      "\n",
      "Epoch: 53\n",
      "\n",
      "Epoch: 53, Loss: 0.151, Acc: 94.778%\n",
      "\n",
      "Test Loss: 0.256, Acc: 92.010%\n",
      "\n",
      "Epoch: 54\n",
      "\n",
      "Epoch: 54, Loss: 0.149, Acc: 94.766%\n",
      "\n",
      "Test Loss: 0.304, Acc: 90.930%\n",
      "\n",
      "Epoch: 55\n",
      "\n",
      "Epoch: 55, Loss: 0.147, Acc: 94.944%\n",
      "\n",
      "Test Loss: 0.262, Acc: 91.510%\n",
      "\n",
      "Epoch: 56\n",
      "\n",
      "Epoch: 56, Loss: 0.139, Acc: 95.174%\n",
      "\n",
      "Test Loss: 0.328, Acc: 90.440%\n",
      "\n",
      "Epoch: 57\n",
      "\n",
      "Epoch: 57, Loss: 0.140, Acc: 95.080%\n",
      "\n",
      "Test Loss: 0.318, Acc: 90.680%\n",
      "\n",
      "Epoch: 58\n",
      "\n",
      "Epoch: 58, Loss: 0.143, Acc: 95.000%\n",
      "\n",
      "Test Loss: 0.278, Acc: 91.640%\n",
      "\n",
      "Epoch: 59\n",
      "\n",
      "Epoch: 59, Loss: 0.138, Acc: 95.226%\n",
      "\n",
      "Test Loss: 0.273, Acc: 92.080%\n",
      "\n",
      "Epoch: 60\n",
      "\n",
      "Epoch: 60, Loss: 0.137, Acc: 95.138%\n",
      "\n",
      "Test Loss: 0.259, Acc: 92.020%\n",
      "\n",
      "Epoch: 61\n",
      "\n",
      "Epoch: 61, Loss: 0.135, Acc: 95.222%\n",
      "\n",
      "Test Loss: 0.265, Acc: 91.980%\n",
      "\n",
      "Epoch: 62\n",
      "\n",
      "Epoch: 62, Loss: 0.136, Acc: 95.224%\n",
      "\n",
      "Test Loss: 0.302, Acc: 91.330%\n",
      "\n",
      "Epoch: 63\n",
      "\n",
      "Epoch: 63, Loss: 0.134, Acc: 95.292%\n",
      "\n",
      "Test Loss: 0.301, Acc: 91.270%\n",
      "\n",
      "Epoch: 64\n",
      "\n",
      "Epoch: 64, Loss: 0.137, Acc: 95.260%\n",
      "\n",
      "Test Loss: 0.315, Acc: 90.560%\n",
      "\n",
      "Epoch: 65\n",
      "\n",
      "Epoch: 65, Loss: 0.129, Acc: 95.592%\n",
      "\n",
      "Test Loss: 0.321, Acc: 90.540%\n",
      "\n",
      "Epoch: 66\n",
      "\n",
      "Epoch: 66, Loss: 0.129, Acc: 95.510%\n",
      "\n",
      "Test Loss: 0.289, Acc: 91.370%\n",
      "\n",
      "Epoch: 67\n",
      "\n",
      "Epoch: 67, Loss: 0.132, Acc: 95.350%\n",
      "\n",
      "Test Loss: 0.295, Acc: 91.460%\n",
      "\n",
      "Epoch: 68\n",
      "\n",
      "Epoch: 68, Loss: 0.127, Acc: 95.572%\n",
      "\n",
      "Test Loss: 0.288, Acc: 91.600%\n",
      "\n",
      "Epoch: 69\n",
      "\n",
      "Epoch: 69, Loss: 0.133, Acc: 95.504%\n",
      "\n",
      "Test Loss: 0.266, Acc: 91.980%\n",
      "\n",
      "Epoch: 70\n",
      "\n",
      "Epoch: 70, Loss: 0.127, Acc: 95.596%\n",
      "\n",
      "Test Loss: 0.334, Acc: 90.650%\n",
      "\n",
      "Epoch: 71\n",
      "\n",
      "Epoch: 71, Loss: 0.123, Acc: 95.832%\n",
      "\n",
      "Test Loss: 0.302, Acc: 91.340%\n",
      "\n",
      "Epoch: 72\n",
      "\n",
      "Epoch: 72, Loss: 0.133, Acc: 95.378%\n",
      "\n",
      "Test Loss: 0.268, Acc: 92.180%\n",
      "\n",
      "Epoch: 73\n",
      "\n",
      "Epoch: 73, Loss: 0.119, Acc: 95.792%\n",
      "\n",
      "Test Loss: 0.261, Acc: 92.020%\n",
      "\n",
      "Epoch: 74\n",
      "\n",
      "Epoch: 74, Loss: 0.126, Acc: 95.746%\n",
      "\n",
      "Test Loss: 0.278, Acc: 91.770%\n",
      "\n",
      "Epoch: 75\n",
      "\n",
      "Epoch: 75, Loss: 0.123, Acc: 95.746%\n",
      "\n",
      "Test Loss: 0.258, Acc: 92.090%\n",
      "\n",
      "Epoch: 76\n",
      "\n",
      "Epoch: 76, Loss: 0.120, Acc: 95.754%\n",
      "\n",
      "Test Loss: 0.283, Acc: 91.880%\n",
      "\n",
      "Epoch: 77\n",
      "\n",
      "Epoch: 77, Loss: 0.122, Acc: 95.734%\n",
      "\n",
      "Test Loss: 0.262, Acc: 92.240%\n",
      "\n",
      "Epoch: 78\n",
      "\n",
      "Epoch: 78, Loss: 0.123, Acc: 95.720%\n",
      "\n",
      "Test Loss: 0.240, Acc: 92.820%\n",
      "\n",
      "Epoch: 79\n",
      "\n",
      "Epoch: 79, Loss: 0.117, Acc: 95.884%\n",
      "\n",
      "Test Loss: 0.274, Acc: 91.540%\n",
      "\n",
      "Epoch: 80\n",
      "\n",
      "Epoch: 80, Loss: 0.122, Acc: 95.702%\n",
      "\n",
      "Test Loss: 0.296, Acc: 91.270%\n",
      "\n",
      "Epoch: 81\n",
      "\n",
      "Epoch: 81, Loss: 0.073, Acc: 97.614%\n",
      "\n",
      "Test Loss: 0.190, Acc: 94.140%\n",
      "\n",
      "Epoch: 82\n",
      "\n",
      "Epoch: 82, Loss: 0.052, Acc: 98.456%\n",
      "\n",
      "Test Loss: 0.187, Acc: 94.290%\n",
      "\n",
      "Epoch: 83\n",
      "\n",
      "Epoch: 83, Loss: 0.045, Acc: 98.662%\n",
      "\n",
      "Test Loss: 0.185, Acc: 94.370%\n",
      "\n",
      "Epoch: 84\n",
      "\n",
      "Epoch: 84, Loss: 0.041, Acc: 98.808%\n",
      "\n",
      "Test Loss: 0.188, Acc: 94.340%\n",
      "\n",
      "Epoch: 85\n",
      "\n",
      "Epoch: 85, Loss: 0.038, Acc: 98.830%\n",
      "\n",
      "Test Loss: 0.186, Acc: 94.440%\n",
      "\n",
      "Epoch: 86\n",
      "\n",
      "Epoch: 86, Loss: 0.037, Acc: 98.890%\n",
      "\n",
      "Test Loss: 0.188, Acc: 94.640%\n",
      "\n",
      "Epoch: 87\n",
      "\n",
      "Epoch: 87, Loss: 0.033, Acc: 99.072%\n",
      "\n",
      "Test Loss: 0.186, Acc: 94.710%\n",
      "\n",
      "Epoch: 88\n",
      "\n",
      "Epoch: 88, Loss: 0.030, Acc: 99.136%\n",
      "\n",
      "Test Loss: 0.192, Acc: 94.340%\n",
      "\n",
      "Epoch: 89\n",
      "\n",
      "Epoch: 89, Loss: 0.032, Acc: 99.092%\n",
      "\n",
      "Test Loss: 0.189, Acc: 94.590%\n",
      "\n",
      "Epoch: 90\n",
      "\n",
      "Epoch: 90, Loss: 0.030, Acc: 99.146%\n",
      "\n",
      "Test Loss: 0.191, Acc: 94.630%\n",
      "\n",
      "Epoch: 91\n",
      "\n",
      "Epoch: 91, Loss: 0.030, Acc: 99.082%\n",
      "\n",
      "Test Loss: 0.192, Acc: 94.600%\n",
      "\n",
      "Epoch: 92\n",
      "\n",
      "Epoch: 92, Loss: 0.029, Acc: 99.178%\n",
      "\n",
      "Test Loss: 0.191, Acc: 94.740%\n",
      "\n",
      "Epoch: 93\n",
      "\n",
      "Epoch: 93, Loss: 0.027, Acc: 99.234%\n",
      "\n",
      "Test Loss: 0.191, Acc: 94.650%\n",
      "\n",
      "Epoch: 94\n",
      "\n",
      "Epoch: 94, Loss: 0.027, Acc: 99.202%\n",
      "\n",
      "Test Loss: 0.193, Acc: 94.700%\n",
      "\n",
      "Epoch: 95\n",
      "\n",
      "Epoch: 95, Loss: 0.026, Acc: 99.290%\n",
      "\n",
      "Test Loss: 0.188, Acc: 94.760%\n",
      "\n",
      "Epoch: 96\n",
      "\n",
      "Epoch: 96, Loss: 0.025, Acc: 99.284%\n",
      "\n",
      "Test Loss: 0.190, Acc: 94.850%\n",
      "\n",
      "Epoch: 97\n",
      "\n",
      "Epoch: 97, Loss: 0.026, Acc: 99.246%\n",
      "\n",
      "Test Loss: 0.193, Acc: 94.610%\n",
      "\n",
      "Epoch: 98\n",
      "\n",
      "Epoch: 98, Loss: 0.024, Acc: 99.310%\n",
      "\n",
      "Test Loss: 0.193, Acc: 94.840%\n",
      "\n",
      "Epoch: 99\n",
      "\n",
      "Epoch: 99, Loss: 0.023, Acc: 99.360%\n",
      "\n",
      "Test Loss: 0.197, Acc: 94.740%\n",
      "\n",
      "Epoch: 100\n",
      "\n",
      "Epoch: 100, Loss: 0.023, Acc: 99.354%\n",
      "\n",
      "Test Loss: 0.196, Acc: 94.740%\n",
      "\n",
      "Epoch: 101\n",
      "\n",
      "Epoch: 101, Loss: 0.022, Acc: 99.344%\n",
      "\n",
      "Test Loss: 0.192, Acc: 94.910%\n",
      "\n",
      "Epoch: 102\n",
      "\n",
      "Epoch: 102, Loss: 0.022, Acc: 99.354%\n",
      "\n",
      "Test Loss: 0.194, Acc: 94.840%\n",
      "\n",
      "Epoch: 103\n",
      "\n",
      "Epoch: 103, Loss: 0.022, Acc: 99.358%\n",
      "\n",
      "Test Loss: 0.192, Acc: 94.780%\n",
      "\n",
      "Epoch: 104\n",
      "\n",
      "Epoch: 104, Loss: 0.021, Acc: 99.432%\n",
      "\n",
      "Test Loss: 0.190, Acc: 94.840%\n",
      "\n",
      "Epoch: 105\n",
      "\n",
      "Epoch: 105, Loss: 0.021, Acc: 99.430%\n",
      "\n",
      "Test Loss: 0.193, Acc: 94.870%\n",
      "\n",
      "Epoch: 106\n",
      "\n",
      "Epoch: 106, Loss: 0.022, Acc: 99.386%\n",
      "\n",
      "Test Loss: 0.196, Acc: 94.730%\n",
      "\n",
      "Epoch: 107\n",
      "\n",
      "Epoch: 107, Loss: 0.022, Acc: 99.374%\n",
      "\n",
      "Test Loss: 0.196, Acc: 94.750%\n",
      "\n",
      "Epoch: 108\n",
      "\n",
      "Epoch: 108, Loss: 0.021, Acc: 99.390%\n",
      "\n",
      "Test Loss: 0.191, Acc: 94.740%\n",
      "\n",
      "Epoch: 109\n",
      "\n",
      "Epoch: 109, Loss: 0.020, Acc: 99.474%\n",
      "\n",
      "Test Loss: 0.201, Acc: 94.770%\n",
      "\n",
      "Epoch: 110\n",
      "\n",
      "Epoch: 110, Loss: 0.019, Acc: 99.424%\n",
      "\n",
      "Test Loss: 0.200, Acc: 94.700%\n",
      "\n",
      "Epoch: 111\n",
      "\n",
      "Epoch: 111, Loss: 0.020, Acc: 99.406%\n",
      "\n",
      "Test Loss: 0.201, Acc: 94.660%\n",
      "\n",
      "Epoch: 112\n",
      "\n",
      "Epoch: 112, Loss: 0.019, Acc: 99.462%\n",
      "\n",
      "Test Loss: 0.199, Acc: 94.760%\n",
      "\n",
      "Epoch: 113\n",
      "\n",
      "Epoch: 113, Loss: 0.020, Acc: 99.416%\n",
      "\n",
      "Test Loss: 0.196, Acc: 94.780%\n",
      "\n",
      "Epoch: 114\n",
      "\n",
      "Epoch: 114, Loss: 0.019, Acc: 99.466%\n",
      "\n",
      "Test Loss: 0.195, Acc: 94.850%\n",
      "\n",
      "Epoch: 115\n",
      "\n",
      "Epoch: 115, Loss: 0.018, Acc: 99.516%\n",
      "\n",
      "Test Loss: 0.195, Acc: 94.850%\n",
      "\n",
      "Epoch: 116\n",
      "\n",
      "Epoch: 116, Loss: 0.018, Acc: 99.468%\n",
      "\n",
      "Test Loss: 0.197, Acc: 94.770%\n",
      "\n",
      "Epoch: 117\n",
      "\n",
      "Epoch: 117, Loss: 0.019, Acc: 99.472%\n",
      "\n",
      "Test Loss: 0.196, Acc: 94.860%\n",
      "\n",
      "Epoch: 118\n",
      "\n",
      "Epoch: 118, Loss: 0.017, Acc: 99.512%\n",
      "\n",
      "Test Loss: 0.198, Acc: 94.730%\n",
      "\n",
      "Epoch: 119\n",
      "\n",
      "Epoch: 119, Loss: 0.018, Acc: 99.476%\n",
      "\n",
      "Test Loss: 0.199, Acc: 94.710%\n",
      "\n",
      "Epoch: 120\n",
      "\n",
      "Epoch: 120, Loss: 0.018, Acc: 99.456%\n",
      "\n",
      "Test Loss: 0.199, Acc: 94.810%\n",
      "\n",
      "Epoch: 121\n",
      "\n",
      "Epoch: 121, Loss: 0.017, Acc: 99.494%\n",
      "\n",
      "Test Loss: 0.198, Acc: 94.960%\n",
      "\n",
      "Epoch: 122\n",
      "\n",
      "Epoch: 122, Loss: 0.017, Acc: 99.534%\n",
      "\n",
      "Test Loss: 0.199, Acc: 94.820%\n",
      "\n",
      "Epoch: 123\n",
      "\n",
      "Epoch: 123, Loss: 0.018, Acc: 99.514%\n",
      "\n",
      "Test Loss: 0.195, Acc: 94.920%\n",
      "\n",
      "Epoch: 124\n",
      "\n",
      "Epoch: 124, Loss: 0.017, Acc: 99.490%\n",
      "\n",
      "Test Loss: 0.199, Acc: 94.920%\n",
      "\n",
      "Epoch: 125\n",
      "\n",
      "Epoch: 125, Loss: 0.017, Acc: 99.516%\n",
      "\n",
      "Test Loss: 0.201, Acc: 94.800%\n",
      "\n",
      "Epoch: 126\n",
      "\n",
      "Epoch: 126, Loss: 0.016, Acc: 99.540%\n",
      "\n",
      "Test Loss: 0.200, Acc: 94.750%\n",
      "\n",
      "Epoch: 127\n",
      "\n",
      "Epoch: 127, Loss: 0.016, Acc: 99.548%\n",
      "\n",
      "Test Loss: 0.203, Acc: 94.850%\n",
      "\n",
      "Epoch: 128\n",
      "\n",
      "Epoch: 128, Loss: 0.017, Acc: 99.536%\n",
      "\n",
      "Test Loss: 0.202, Acc: 94.800%\n",
      "\n",
      "Epoch: 129\n",
      "\n",
      "Epoch: 129, Loss: 0.016, Acc: 99.552%\n",
      "\n",
      "Test Loss: 0.202, Acc: 94.820%\n",
      "\n",
      "Epoch: 130\n",
      "\n",
      "Epoch: 130, Loss: 0.016, Acc: 99.598%\n",
      "\n",
      "Test Loss: 0.206, Acc: 94.810%\n",
      "\n",
      "Epoch: 131\n",
      "\n",
      "Epoch: 131, Loss: 0.016, Acc: 99.532%\n",
      "\n",
      "Test Loss: 0.209, Acc: 94.710%\n",
      "\n",
      "Epoch: 132\n",
      "\n",
      "Epoch: 132, Loss: 0.015, Acc: 99.542%\n",
      "\n",
      "Test Loss: 0.203, Acc: 94.960%\n",
      "\n",
      "Epoch: 133\n",
      "\n",
      "Epoch: 133, Loss: 0.016, Acc: 99.536%\n",
      "\n",
      "Test Loss: 0.203, Acc: 94.930%\n",
      "\n",
      "Epoch: 134\n",
      "\n",
      "Epoch: 134, Loss: 0.017, Acc: 99.496%\n",
      "\n",
      "Test Loss: 0.204, Acc: 94.830%\n",
      "\n",
      "Epoch: 135\n",
      "\n",
      "Epoch: 135, Loss: 0.016, Acc: 99.504%\n",
      "\n",
      "Test Loss: 0.205, Acc: 94.940%\n",
      "\n",
      "Epoch: 136\n",
      "\n",
      "Epoch: 136, Loss: 0.015, Acc: 99.584%\n",
      "\n",
      "Test Loss: 0.202, Acc: 95.100%\n",
      "\n",
      "Epoch: 137\n",
      "\n",
      "Epoch: 137, Loss: 0.015, Acc: 99.596%\n",
      "\n",
      "Test Loss: 0.203, Acc: 95.020%\n",
      "\n",
      "Epoch: 138\n",
      "\n",
      "Epoch: 138, Loss: 0.015, Acc: 99.588%\n",
      "\n",
      "Test Loss: 0.200, Acc: 95.030%\n",
      "\n",
      "Epoch: 139\n",
      "\n",
      "Epoch: 139, Loss: 0.016, Acc: 99.524%\n",
      "\n",
      "Test Loss: 0.204, Acc: 94.920%\n",
      "\n",
      "Epoch: 140\n",
      "\n",
      "Epoch: 140, Loss: 0.014, Acc: 99.590%\n",
      "\n",
      "Test Loss: 0.205, Acc: 94.820%\n",
      "\n",
      "Epoch: 141\n",
      "\n",
      "Epoch: 141, Loss: 0.016, Acc: 99.546%\n",
      "\n",
      "Test Loss: 0.208, Acc: 94.930%\n",
      "\n",
      "Epoch: 142\n",
      "\n",
      "Epoch: 142, Loss: 0.014, Acc: 99.600%\n",
      "\n",
      "Test Loss: 0.202, Acc: 95.050%\n",
      "\n",
      "Epoch: 143\n",
      "\n",
      "Epoch: 143, Loss: 0.014, Acc: 99.610%\n",
      "\n",
      "Test Loss: 0.205, Acc: 94.950%\n",
      "\n",
      "Epoch: 144\n",
      "\n",
      "Epoch: 144, Loss: 0.014, Acc: 99.616%\n",
      "\n",
      "Test Loss: 0.203, Acc: 94.980%\n",
      "\n",
      "Epoch: 145\n",
      "\n",
      "Epoch: 145, Loss: 0.016, Acc: 99.558%\n",
      "\n",
      "Test Loss: 0.205, Acc: 94.980%\n",
      "\n",
      "Epoch: 146\n",
      "\n",
      "Epoch: 146, Loss: 0.014, Acc: 99.642%\n",
      "\n",
      "Test Loss: 0.201, Acc: 94.990%\n",
      "\n",
      "Epoch: 147\n",
      "\n",
      "Epoch: 147, Loss: 0.014, Acc: 99.610%\n",
      "\n",
      "Test Loss: 0.201, Acc: 95.180%\n",
      "\n",
      "Epoch: 148\n",
      "\n",
      "Epoch: 148, Loss: 0.015, Acc: 99.574%\n",
      "\n",
      "Test Loss: 0.203, Acc: 94.860%\n",
      "\n",
      "Epoch: 149\n",
      "\n",
      "Epoch: 149, Loss: 0.015, Acc: 99.560%\n",
      "\n",
      "Test Loss: 0.201, Acc: 95.010%\n",
      "\n",
      "Epoch: 150\n",
      "\n",
      "Epoch: 150, Loss: 0.014, Acc: 99.610%\n",
      "\n",
      "Test Loss: 0.203, Acc: 94.940%\n",
      "\n",
      "Epoch: 151\n",
      "\n",
      "Epoch: 151, Loss: 0.013, Acc: 99.586%\n",
      "\n",
      "Test Loss: 0.201, Acc: 94.980%\n",
      "\n",
      "Epoch: 152\n",
      "\n",
      "Epoch: 152, Loss: 0.013, Acc: 99.634%\n",
      "\n",
      "Test Loss: 0.199, Acc: 94.960%\n",
      "\n",
      "Epoch: 153\n",
      "\n",
      "Epoch: 153, Loss: 0.013, Acc: 99.638%\n",
      "\n",
      "Test Loss: 0.200, Acc: 94.920%\n",
      "\n",
      "Epoch: 154\n",
      "\n",
      "Epoch: 154, Loss: 0.013, Acc: 99.656%\n",
      "\n",
      "Test Loss: 0.200, Acc: 94.940%\n",
      "\n",
      "Epoch: 155\n",
      "\n",
      "Epoch: 155, Loss: 0.013, Acc: 99.634%\n",
      "\n",
      "Test Loss: 0.200, Acc: 95.000%\n",
      "\n",
      "Epoch: 156\n",
      "\n",
      "Epoch: 156, Loss: 0.012, Acc: 99.700%\n",
      "\n",
      "Test Loss: 0.201, Acc: 94.930%\n",
      "\n",
      "Epoch: 157\n",
      "\n",
      "Epoch: 157, Loss: 0.012, Acc: 99.642%\n",
      "\n",
      "Test Loss: 0.200, Acc: 94.900%\n",
      "\n",
      "Epoch: 158\n",
      "\n",
      "Epoch: 158, Loss: 0.012, Acc: 99.704%\n",
      "\n",
      "Test Loss: 0.200, Acc: 95.000%\n",
      "\n",
      "Epoch: 159\n",
      "\n",
      "Epoch: 159, Loss: 0.012, Acc: 99.668%\n",
      "\n",
      "Test Loss: 0.199, Acc: 95.040%\n",
      "\n",
      "Epoch: 160\n",
      "\n",
      "Epoch: 160, Loss: 0.011, Acc: 99.730%\n",
      "\n",
      "Test Loss: 0.198, Acc: 94.930%\n",
      "\n",
      "Epoch: 161\n",
      "\n",
      "Epoch: 161, Loss: 0.012, Acc: 99.672%\n",
      "\n",
      "Test Loss: 0.200, Acc: 94.920%\n",
      "\n",
      "Epoch: 162\n",
      "\n",
      "Epoch: 162, Loss: 0.011, Acc: 99.692%\n",
      "\n",
      "Test Loss: 0.202, Acc: 95.020%\n",
      "\n",
      "Epoch: 163\n",
      "\n",
      "Epoch: 163, Loss: 0.012, Acc: 99.654%\n",
      "\n",
      "Test Loss: 0.200, Acc: 94.990%\n",
      "\n",
      "Epoch: 164\n",
      "\n",
      "Epoch: 164, Loss: 0.012, Acc: 99.698%\n",
      "\n",
      "Test Loss: 0.200, Acc: 94.940%\n",
      "\n",
      "Epoch: 165\n",
      "\n",
      "Epoch: 165, Loss: 0.012, Acc: 99.720%\n",
      "\n",
      "Test Loss: 0.200, Acc: 95.060%\n",
      "\n",
      "Epoch: 166\n",
      "\n",
      "Epoch: 166, Loss: 0.011, Acc: 99.718%\n",
      "\n",
      "Test Loss: 0.201, Acc: 95.030%\n",
      "\n",
      "Epoch: 167\n",
      "\n",
      "Epoch: 167, Loss: 0.011, Acc: 99.710%\n",
      "\n",
      "Test Loss: 0.199, Acc: 94.990%\n",
      "\n",
      "Epoch: 168\n",
      "\n",
      "Epoch: 168, Loss: 0.012, Acc: 99.686%\n",
      "\n",
      "Test Loss: 0.198, Acc: 95.030%\n",
      "\n",
      "Epoch: 169\n",
      "\n",
      "Epoch: 169, Loss: 0.012, Acc: 99.684%\n",
      "\n",
      "Test Loss: 0.199, Acc: 95.030%\n",
      "\n",
      "Epoch: 170\n",
      "\n",
      "Epoch: 170, Loss: 0.011, Acc: 99.726%\n",
      "\n",
      "Test Loss: 0.200, Acc: 94.980%\n",
      "\n",
      "Epoch: 171\n",
      "\n",
      "Epoch: 171, Loss: 0.011, Acc: 99.680%\n",
      "\n",
      "Test Loss: 0.197, Acc: 94.980%\n",
      "\n",
      "Epoch: 172\n",
      "\n",
      "Epoch: 172, Loss: 0.011, Acc: 99.714%\n",
      "\n",
      "Test Loss: 0.197, Acc: 95.050%\n",
      "\n",
      "Epoch: 173\n",
      "\n",
      "Epoch: 173, Loss: 0.011, Acc: 99.696%\n",
      "\n",
      "Test Loss: 0.198, Acc: 95.040%\n",
      "\n",
      "Epoch: 174\n",
      "\n",
      "Epoch: 174, Loss: 0.011, Acc: 99.682%\n",
      "\n",
      "Test Loss: 0.198, Acc: 95.080%\n",
      "\n",
      "Epoch: 175\n",
      "\n",
      "Epoch: 175, Loss: 0.012, Acc: 99.672%\n",
      "\n",
      "Test Loss: 0.198, Acc: 94.960%\n",
      "\n",
      "Epoch: 176\n",
      "\n",
      "Epoch: 176, Loss: 0.011, Acc: 99.718%\n",
      "\n",
      "Test Loss: 0.199, Acc: 95.010%\n",
      "\n",
      "Epoch: 177\n",
      "\n",
      "Epoch: 177, Loss: 0.011, Acc: 99.696%\n",
      "\n",
      "Test Loss: 0.198, Acc: 95.140%\n",
      "\n",
      "Epoch: 178\n",
      "\n",
      "Epoch: 178, Loss: 0.011, Acc: 99.688%\n",
      "\n",
      "Test Loss: 0.199, Acc: 95.010%\n",
      "\n",
      "Epoch: 179\n",
      "\n",
      "Epoch: 179, Loss: 0.011, Acc: 99.720%\n",
      "\n",
      "Test Loss: 0.199, Acc: 94.970%\n",
      "\n",
      "Epoch: 180\n",
      "\n",
      "Epoch: 180, Loss: 0.011, Acc: 99.710%\n",
      "\n",
      "Test Loss: 0.202, Acc: 94.990%\n",
      "\n",
      "Epoch: 181\n",
      "\n",
      "Epoch: 181, Loss: 0.012, Acc: 99.686%\n",
      "\n",
      "Test Loss: 0.199, Acc: 94.990%\n",
      "\n",
      "Epoch: 182\n",
      "\n",
      "Epoch: 182, Loss: 0.011, Acc: 99.690%\n",
      "\n",
      "Test Loss: 0.199, Acc: 95.110%\n",
      "\n",
      "Epoch: 183\n",
      "\n",
      "Epoch: 183, Loss: 0.011, Acc: 99.734%\n",
      "\n",
      "Test Loss: 0.200, Acc: 95.010%\n",
      "\n",
      "Epoch: 184\n",
      "\n",
      "Epoch: 184, Loss: 0.010, Acc: 99.760%\n",
      "\n",
      "Test Loss: 0.198, Acc: 95.060%\n",
      "\n",
      "Epoch: 185\n",
      "\n",
      "Epoch: 185, Loss: 0.010, Acc: 99.752%\n",
      "\n",
      "Test Loss: 0.199, Acc: 94.960%\n",
      "\n",
      "Epoch: 186\n",
      "\n",
      "Epoch: 186, Loss: 0.011, Acc: 99.724%\n",
      "\n",
      "Test Loss: 0.201, Acc: 95.040%\n",
      "\n",
      "Epoch: 187\n",
      "\n",
      "Epoch: 187, Loss: 0.010, Acc: 99.742%\n",
      "\n",
      "Test Loss: 0.198, Acc: 95.040%\n",
      "\n",
      "Epoch: 188\n",
      "\n",
      "Epoch: 188, Loss: 0.011, Acc: 99.720%\n",
      "\n",
      "Test Loss: 0.198, Acc: 95.000%\n",
      "\n",
      "Epoch: 189\n",
      "\n",
      "Epoch: 189, Loss: 0.009, Acc: 99.758%\n",
      "\n",
      "Test Loss: 0.198, Acc: 95.040%\n",
      "\n",
      "Epoch: 190\n",
      "\n",
      "Epoch: 190, Loss: 0.012, Acc: 99.694%\n",
      "\n",
      "Test Loss: 0.199, Acc: 95.140%\n",
      "\n",
      "Epoch: 191\n",
      "\n",
      "Epoch: 191, Loss: 0.010, Acc: 99.768%\n",
      "\n",
      "Test Loss: 0.198, Acc: 95.150%\n",
      "\n",
      "Epoch: 192\n",
      "\n",
      "Epoch: 192, Loss: 0.010, Acc: 99.726%\n",
      "\n",
      "Test Loss: 0.199, Acc: 95.070%\n",
      "\n",
      "Epoch: 193\n",
      "\n",
      "Epoch: 193, Loss: 0.010, Acc: 99.732%\n",
      "\n",
      "Test Loss: 0.199, Acc: 95.060%\n",
      "\n",
      "Epoch: 194\n",
      "\n",
      "Epoch: 194, Loss: 0.011, Acc: 99.716%\n",
      "\n",
      "Test Loss: 0.197, Acc: 95.080%\n",
      "\n",
      "Epoch: 195\n",
      "\n",
      "Epoch: 195, Loss: 0.011, Acc: 99.704%\n",
      "\n",
      "Test Loss: 0.197, Acc: 95.120%\n",
      "\n",
      "Epoch: 196\n",
      "\n",
      "Epoch: 196, Loss: 0.011, Acc: 99.702%\n",
      "\n",
      "Test Loss: 0.198, Acc: 95.100%\n",
      "\n",
      "Epoch: 197\n",
      "\n",
      "Epoch: 197, Loss: 0.010, Acc: 99.742%\n",
      "\n",
      "Test Loss: 0.197, Acc: 95.110%\n",
      "\n",
      "Epoch: 198\n",
      "\n",
      "Epoch: 198, Loss: 0.011, Acc: 99.714%\n",
      "\n",
      "Test Loss: 0.196, Acc: 95.090%\n",
      "\n",
      "Epoch: 199\n",
      "\n",
      "Epoch: 199, Loss: 0.010, Acc: 99.730%\n",
      "\n",
      "Test Loss: 0.197, Acc: 95.010%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      test_loss \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 199\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy 95.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      test_loss 0.19657\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 99.73\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0.01028\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run \u001b[33mearnest-sweep-1\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/kavyagupta/dl-mini-353-cutout/runs/ae12ytgm\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at: \u001b[34m\u001b[4mhttps://wandb.ai/kavyagupta/dl-mini-353-cutout\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240412_230234-ae12ytgm/logs\u001b[0m\n",
      "Run ae12ytgm errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_23/1604683654.py\", line 88, in trainwandb\n",
      "    plot_train_test_accuracy(train_accuracies, test_accuracies, train_losses, test_losses)\n",
      "  File \"/tmp/ipykernel_23/4014860157.py\", line 5, in plot_train_test_accuracy\n",
      "    plt.figure(figsize=(12, 5))\n",
      "TypeError: 'module' object is not callable\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run ae12ytgm errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_23/1604683654.py\", line 88, in trainwandb\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     plot_train_test_accuracy(train_accuracies, test_accuracies, train_losses, test_losses)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_23/4014860157.py\", line 5, in plot_train_test_accuracy\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     plt.figure(figsize=(12, 5))\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m TypeError: 'module' object is not callable\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n"
     ]
    }
   ],
   "source": [
    "if use_wandb==1:\n",
    "  wandb.agent(sweep_id, trainwandb, count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5ef9ef",
   "metadata": {
    "id": "hOd-vV77tm1O",
    "papermill": {
     "duration": 0.045647,
     "end_time": "2024-04-13T00:37:58.059440",
     "exception": false,
     "start_time": "2024-04-13T00:37:58.013793",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "If running without WANDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c3b438",
   "metadata": {
    "id": "Jq2DbaiTw_r9",
    "papermill": {
     "duration": 0.043466,
     "end_time": "2024-04-13T00:37:58.162060",
     "exception": false,
     "start_time": "2024-04-13T00:37:58.118594",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Loading pkl model if needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b1031748",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T00:37:58.252133Z",
     "iopub.status.busy": "2024-04-13T00:37:58.251156Z",
     "iopub.status.idle": "2024-04-13T00:37:58.256397Z",
     "shell.execute_reply": "2024-04-13T00:37:58.255483Z"
    },
    "id": "gNMjWV-sw9dG",
    "papermill": {
     "duration": 0.052234,
     "end_time": "2024-04-13T00:37:58.258307",
     "exception": false,
     "start_time": "2024-04-13T00:37:58.206073",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if load_model == 1:\n",
    "  loadFilename = load_model_name\n",
    "  checkpoint = torch.load(loadFilename)\n",
    "  net = checkpoint['net']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0bba9bc5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T00:37:58.346263Z",
     "iopub.status.busy": "2024-04-13T00:37:58.345930Z",
     "iopub.status.idle": "2024-04-13T00:37:58.357211Z",
     "shell.execute_reply": "2024-04-13T00:37:58.356387Z"
    },
    "id": "bSZaWr8wtpAd",
    "papermill": {
     "duration": 0.057518,
     "end_time": "2024-04-13T00:37:58.359099",
     "exception": false,
     "start_time": "2024-04-13T00:37:58.301581",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if use_wandb==0:\n",
    "  do_clipping = 0\n",
    "  if args_optimizer=='sgdn':\n",
    "    optimizer = optim.SGD(net.parameters(), lr=args_lr, momentum=0.9, weight_decay=args_weight_decay, nesterov=True)\n",
    "  elif args_optimizer=='adadelta':\n",
    "    optimizer = optim.Adadelta(net.parameters(), lr=args_lr, weight_decay=args_weight_decay)\n",
    "  elif args_optimizer=='adadelta-clipping':\n",
    "    optimizer = optim.Adadelta(net.parameters(), lr=args_lr, weight_decay=args_weight_decay)\n",
    "    do_clipping = 1\n",
    "  else: #sgd\n",
    "    optimizer = optim.SGD(net.parameters(), lr=args_lr, momentum=0.9, weight_decay=args_weight_decay)\n",
    "\n",
    "  scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=t_max)\n",
    "  scheduler.step()\n",
    "\n",
    "  # number_of_epochs = 100\n",
    "\n",
    "  train_accuracies = []\n",
    "  test_accuracies = []\n",
    "  train_losses = []\n",
    "  test_losses = []\n",
    "\n",
    "  if earlystop:\n",
    "    early_stopper = EarlyStopper(patience=earlystop_patience, min_delta=earlystop_mindelta)\n",
    "  for epoch in range(start_epoch, start_epoch+number_of_epochs):\n",
    "      train_acc, train_loss = train(epoch, optimizer, net, do_clipping)\n",
    "      test_acc, test_loss = test(optimizer, net)\n",
    "\n",
    "      train_accuracies.append(train_acc)\n",
    "      test_accuracies.append(test_acc)\n",
    "      train_losses.append(train_loss)\n",
    "      test_losses.append(test_loss)\n",
    "\n",
    "      if(epoch%save_freq==0):\n",
    "          saveModel(net, epoch)\n",
    "          #test()\n",
    "      if earlystop==1:\n",
    "        if early_stopper.early_stop(test_acc) and test_acc>earlystop_threshold:\n",
    "          wandb.log({\"early_stop\": epoch})\n",
    "          break\n",
    "\n",
    "  saveModel(net, number_of_epochs)\n",
    "\n",
    "  # Plot train vs test accuracy graph\n",
    "  plot_train_test_accuracy(train_accuracies, test_accuracies, train_losses, test_losses)\n",
    "\n",
    "  test(optimizer, net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43af6ba",
   "metadata": {
    "id": "DkUbC4HQCekw",
    "papermill": {
     "duration": 0.043535,
     "end_time": "2024-04-13T00:37:58.446510",
     "exception": false,
     "start_time": "2024-04-13T00:37:58.402975",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Generating CSV file with Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e6923b2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T00:37:58.534459Z",
     "iopub.status.busy": "2024-04-13T00:37:58.534177Z",
     "iopub.status.idle": "2024-04-13T00:37:59.078602Z",
     "shell.execute_reply": "2024-04-13T00:37:59.077583Z"
    },
    "id": "2ECGp3_OCekw",
    "papermill": {
     "duration": 0.590923,
     "end_time": "2024-04-13T00:37:59.080978",
     "exception": false,
     "start_time": "2024-04-13T00:37:58.490055",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, pkl_file, transform=None):\n",
    "        with open(pkl_file, 'rb') as f:\n",
    "            self.data_dict = pickle.load(f)\n",
    "        self.data = self.data_dict[b'data']\n",
    "        self.ids = self.data_dict[b'ids']\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_flat = self.data[idx, :]\n",
    "        img = img_flat.reshape(3, 32, 32) / 255.0  # Convert to float and scale to [0, 1]\n",
    "        img_id = self.ids[idx]\n",
    "        img_tensor = torch.from_numpy(img).float()  # Convert to tensor and ensure type\n",
    "\n",
    "        if self.transform:\n",
    "            img_tensor = self.transform(img_tensor)\n",
    "\n",
    "        return img_tensor, img_id\n",
    "\n",
    "# Define transformations\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# Create dataset and dataloader\n",
    "test_dataset = ''\n",
    "if kaggle==1:\n",
    "  test_dataset = CustomDataset('/kaggle/input/deep-learning-mini-project-spring-24-nyu/cifar_test_nolabels.pkl', transform=test_transform)\n",
    "else:\n",
    "    test_dataset = CustomDataset('cifar_test_nolabels.pkl', transform=test_transform)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=our_batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "79751110",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T00:37:59.171469Z",
     "iopub.status.busy": "2024-04-13T00:37:59.171150Z",
     "iopub.status.idle": "2024-04-13T00:37:59.178605Z",
     "shell.execute_reply": "2024-04-13T00:37:59.177748Z"
    },
    "id": "zeJd9rRHCekw",
    "papermill": {
     "duration": 0.054342,
     "end_time": "2024-04-13T00:37:59.180430",
     "exception": false,
     "start_time": "2024-04-13T00:37:59.126088",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if final_test_create_csv==1:\n",
    "  predicted_labels = []\n",
    "  image_ids = []\n",
    "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "  net.eval()\n",
    "  with torch.no_grad():\n",
    "      for images, ids in test_dataloader:\n",
    "          images = images.to(device)\n",
    "          myoutputs = net(images)\n",
    "          _, mypredicted = myoutputs.max(1)\n",
    "          predicted_labels.extend(mypredicted.cpu().numpy())\n",
    "          image_ids.extend(ids.numpy())\n",
    "\n",
    "  # Create a DataFrame and then save it to CSV\n",
    "  df = pd.DataFrame({\n",
    "      \"ID\": image_ids,\n",
    "      \"Labels\": predicted_labels\n",
    "  })\n",
    "\n",
    "  # Save the DataFrame to a CSV file\n",
    "  if kaggle==1:\n",
    "    df.to_csv('/kaggle/working/predictions.csv', index=False)\n",
    "  else:\n",
    "    df.to_csv('predictions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 8112053,
     "sourceId": 73233,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30683,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5762.619692,
   "end_time": "2024-04-13T00:38:01.753078",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-12T23:01:59.133386",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
